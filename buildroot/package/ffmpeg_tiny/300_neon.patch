diff -uwbr ffmpeg_tiny/configure ffmpeg_tiny_patched/configure
--- ffmpeg_tiny/configure	2009-02-04 20:38:26.000000000 +0100
+++ ffmpeg_tiny_patched/configure	2009-03-20 14:25:12.000000000 +0100
@@ -1828,7 +1828,7 @@
 enabled armvfp  && check_asm armvfp  '"fadds s0, s0, s0"'
 enabled iwmmxt  && check_asm iwmmxt  '"wunpckelub wr6, wr4"'
 enabled mmi     && check_asm mmi     '"lq $2, 0($2)"'
-enabled neon    && check_asm neon    '"vadd.i16 q0, q0, q0"'
+enabled neon    && check_asm neon    '"vadd.i16 q0, q0, q0"' -mfpu=neon
 enabled vis     && check_asm vis     '"pdist %f0, %f0, %f0"' -mcpu=ultrasparc
 
 enabled vis && add_cflags -mcpu=ultrasparc -mtune=ultrasparc
diff -uwbr ffmpeg_tiny/libavcodec/arm/dsputil_neon_s.S ffmpeg_tiny_patched/libavcodec/arm/dsputil_neon_s.S
--- ffmpeg_tiny/libavcodec/arm/dsputil_neon_s.S	2009-02-04 20:21:26.000000000 +0100
+++ ffmpeg_tiny_patched/libavcodec/arm/dsputil_neon_s.S	2009-03-20 16:03:27.000000000 +0100
@@ -665,9 +665,9 @@
         vld1.64         {d4,d5},  [r3,:128]!
         vld1.64         {d6,d7},  [r4,:128], r5
 1:      subs            lr,  lr,  #4
-        vmov            q11, q8
+        vorr            q11, q8, q8
         vmla.f32        d22, d0,  d4
-        vmov            q10, q8
+        vorr            q10, q8, q8
         vmla.f32        d23, d1,  d5
         vrev64.32       q3,  q3
         vmla.f32        d20, d0,  d7
@@ -682,9 +682,9 @@
         vld1.64         {d24,d25},[r3,:128]!
         vmls.f32        d21, d2,  d5
         vld1.64         {d6,d7},  [r4,:128], r5
-        vmov            q1,  q9
+        vorr            q1,  q9, q9
         vrev64.32       q11, q11
-        vmov            q2,  q12
+        vorr            q2,  q12, q12
         vswp            d22, d23
         vst1.64         {d20,d21},[r0,:128]!
         vst1.64         {d22,d23},[ip,:128], r5
diff -uwbr ffmpeg_tiny/libavcodec/arm/h264dsp_neon.S ffmpeg_tiny_patched/libavcodec/arm/h264dsp_neon.S
--- ffmpeg_tiny/libavcodec/arm/h264dsp_neon.S	2009-02-04 20:21:26.000000000 +0100
+++ ffmpeg_tiny_patched/libavcodec/arm/h264dsp_neon.S	2009-03-20 16:08:26.000000000 +0100
@@ -1374,8 +1374,8 @@
         .macro  biweight_16 macs, macd
         vdup.8          d0,  r4
         vdup.8          d1,  r5
-        vmov            q2,  q8
-        vmov            q3,  q8
+        vorr            q2,  q8, q8
+        vorr            q3,  q8, q8
 1:      subs            ip,  ip,  #2
         vld1.8          {d20-d21},[r0,:128], r2
         \macd           q2,  d0,  d20
@@ -1385,9 +1385,9 @@
         \macs           q2,  d1,  d22
         pld             [r1]
         \macs           q3,  d1,  d23
-        vmov            q12, q8
+        vorr            q12, q8, q8
         vld1.8          {d28-d29},[r0,:128], r2
-        vmov            q13, q8
+        vorr            q13, q8, q8
         \macd           q12, d0,  d28
         pld             [r0]
         \macd           q13, d0,  d29
@@ -1403,9 +1403,9 @@
         vshl.s16        q13, q13, q9
         vqmovun.s16     d24, q12
         vqmovun.s16     d25, q13
-        vmov            q3,  q8
+        vorr            q3,  q8, q8
         vst1.8          {d4- d5}, [r6,:128], r2
-        vmov            q2,  q8
+        vorr            q2,  q8, q8
         vst1.8          {d24-d25},[r6,:128], r2
         bne             1b
         pop             {r4-r6, pc}
@@ -1414,8 +1414,8 @@
         .macro  biweight_8 macs, macd
         vdup.8          d0,  r4
         vdup.8          d1,  r5
-        vmov            q1,  q8
-        vmov            q10, q8
+        vorr            q1,  q8, q8
+        vorr            q10, q8, q8
 1:      subs            ip,  ip,  #2
         vld1.8          {d4},[r0,:64], r2
         \macd           q1,  d0,  d4
@@ -1433,9 +1433,9 @@
         vqmovun.s16     d2,  q1
         vshl.s16        q10, q10, q9
         vqmovun.s16     d4,  q10
-        vmov            q10, q8
+        vorr            q10, q8, q8
         vst1.8          {d2},[r6,:64], r2
-        vmov            q1,  q8
+        vorr            q1,  q8, q8
         vst1.8          {d4},[r6,:64], r2
         bne             1b
         pop             {r4-r6, pc}
@@ -1444,8 +1444,8 @@
         .macro  biweight_4 macs, macd
         vdup.8          d0,  r4
         vdup.8          d1,  r5
-        vmov            q1,  q8
-        vmov            q10, q8
+        vorr            q1,  q8, q8
+        vorr            q10, q8, q8
 1:      subs            ip,  ip,  #4
         vld1.32         {d4[0]},[r0,:32], r2
         vld1.32         {d4[1]},[r0,:32], r2
@@ -1468,10 +1468,10 @@
         vqmovun.s16     d2,  q1
         vshl.s16        q10, q10, q9
         vqmovun.s16     d4,  q10
-        vmov            q10, q8
+        vorr            q10, q8, q8
         vst1.32         {d2[0]},[r6,:32], r2
         vst1.32         {d2[1]},[r6,:32], r2
-        vmov            q1,  q8
+        vorr            q1,  q8, q8
         vst1.32         {d4[0]},[r6,:32], r2
         vst1.32         {d4[1]},[r6,:32], r2
         bne             1b
@@ -1595,8 +1595,8 @@
 
         .macro  weight_4 mac
         vdup.8          d0,  r3
-        vmov            q1,  q8
-        vmov            q10, q8
+        vorr            q1,  q8, q8
+        vorr            q10, q8, q8
 1:      subs            ip,  ip,  #4
         vld1.32         {d4[0]},[r0,:32], r1
         vld1.32         {d4[1]},[r0,:32], r1
@@ -1611,10 +1611,10 @@
         vqmovun.s16     d2,  q1
         vshl.s16        q10, q10, q9
         vqmovun.s16     d4,  q10
-        vmov            q10, q8
+        vorr            q10, q8, q8
         vst1.32         {d2[0]},[r4,:32], r1
         vst1.32         {d2[1]},[r4,:32], r1
-        vmov            q1,  q8
+        vorr            q1,  q8, q8
         vst1.32         {d4[0]},[r4,:32], r1
         vst1.32         {d4[1]},[r4,:32], r1
         bne             1b
